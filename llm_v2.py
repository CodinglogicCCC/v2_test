import os
from dotenv import load_dotenv
from pathlib import Path

# .env íŒŒì¼ ëª…ì‹œì ìœ¼ë¡œ ë¶ˆëŸ¬ì˜¤ê¸°
dotenv_path = Path(__file__).resolve().parent / ".env"
load_dotenv(dotenv_path)
import logging
import re
from langchain_core.prompts import ChatPromptTemplate, FewShotPromptTemplate, PromptTemplate
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from config import answer_examples  # FAQ ë°ì´í„° ë¡œë“œ

# ë¡œê·¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)

# ì„¸ì…˜ë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì €ì¥ì†Œ
store = {}

def get_session_history(session_id: str) -> BaseChatMessageHistory:
    """ì„¸ì…˜ IDë³„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê´€ë¦¬"""
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

def get_retriever():
    """Pinecone ë²¡í„° ê²€ìƒ‰ê¸° ìƒì„±"""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise ValueError("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    embedding = OpenAIEmbeddings(
    model="text-embedding-3-large",
    openai_api_key=api_key,
    dimensions=1536  # Pinecone ì¸ë±ìŠ¤ì— ë§ì¶¤
)
    index_name = "v2-test"  # ê¸°ì¡´ Pinecone ì¸ë±ìŠ¤ë¥¼ ìœ ì§€
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)
    return database.as_retriever(search_kwargs={"k": 5})

def get_llm(model: str = "gpt-4o") -> ChatOpenAI:
    """GPT-4o ëª¨ë¸ì„ ì„¤ì •"""
    return ChatOpenAI(model=model, streaming=True)

def get_rag_chain() -> RunnableWithMessageHistory:
    """RAG ì²´ì¸ ìƒì„±"""
    llm = get_llm()
    retriever = get_retriever()

    system_prompt = (
        "ë‹¹ì‹ ì€ ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµì˜ ê°œì¸ì •ë³´ ë³´í˜¸ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
        "ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°˜ë“œì‹œ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. "
        "ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”. "
        "ì¶œì²˜ëŠ” '(ì¶œì²˜: ë¬¸ì„œëª…: [ë¬¸ì„œëª…], í˜ì´ì§€: [í˜ì´ì§€])' í˜•ì‹ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”. "
        "ë¬¸ì„œ ë‚´ìš©ê³¼ ê´€ë ¨ ì—†ëŠ” ì •ë³´ë¥¼ ìƒì„±í•˜ì§€ ë§ˆì„¸ìš”. "
        "ë§Œì•½ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë¶€ì¡±í•˜ë©´ 'ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ë³´ì‹œê² ì–´ìš”?'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n\n"
        "{context}"
    )

    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    return RunnableWithMessageHistory(
        rag_chain, get_session_history,
        input_messages_key="input", history_messages_key="chat_history", output_messages_key="answer",
    )

# FAQ ê¸°ë°˜ Few-shot Prompting ì„¤ì •
example_template = PromptTemplate(
    input_variables=["input", "answer"],
    template="ì§ˆë¬¸: {input}\në‹µë³€: {answer}\n"
)

faq_prompt = FewShotPromptTemplate(
    examples=answer_examples,
    example_prompt=example_template,
    suffix="ì§ˆë¬¸: {query}\në‹µë³€:",
    input_variables=["query"]
)

def normalize_query(user_message: str) -> str:
    """ì‚¬ìš©ì ì…ë ¥ì„ í‘œì¤€í™”í•˜ì—¬ 'ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ'ë¡œ ë³€í™˜"""
    replacements = {
        "ì„œìš¸ê³¼ê¸°ëŒ€": "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ",
        "ê³¼ê¸°ëŒ€": "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ",
        "í•™êµ": "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ"
    }
    
    for key, value in replacements.items():
        user_message = user_message.replace(key, value)
    
    logging.info(f"ğŸ›  ì…ë ¥ ë³€í™˜ë¨: {user_message}")
    return user_message

def check_few_shot_examples(question: str) -> str:
    """Few-shot ë°ì´í„°ì—ì„œ ì •ë‹µì´ ìˆëŠ”ì§€ í™•ì¸"""
    for example in answer_examples:
        if example["input"] == question:
            return example["answer"]
    return None

def validate_answer_with_docs(few_shot_answer: str, retrieved_docs) -> str:
    """Few-shot ë°ì´í„°ì™€ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì¶©ëŒí•˜ëŠ”ì§€ í™•ì¸"""
    for doc in retrieved_docs:
        if few_shot_answer in doc.page_content:
            return few_shot_answer  # ë¬¸ì„œì™€ ë™ì¼í•˜ë©´ Few-shot ìœ ì§€
    return None  # ë¬¸ì„œì™€ ì¶©ëŒí•˜ë©´ ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ë‚´ìš© ì‚¬ìš©

def get_ai_response(user_message: str, session_id: str) -> str:
    """AI ì‘ë‹µ ìƒì„±: RAG ì‹¤íŒ¨ ì‹œ Few-shot Promptingì„ ì‚¬ìš©"""
    try:
        # ì‚¬ìš©ì ì…ë ¥ì„ í‘œì¤€í™”í•˜ì—¬ "ì„œìš¸ê³¼í•™ê¸°ìˆ ëŒ€í•™êµ"ë¡œ ë³€í™˜
        normalized_query = normalize_query(user_message)

        # Few-shotì—ì„œ ë¨¼ì € ë‹µë³€ ì°¾ê¸°
        answer = check_few_shot_examples(normalized_query)

        # ê²€ìƒ‰ëœ ë¬¸ì„œ í™•ì¸
        retriever = get_retriever()
        retrieved_docs = retriever.invoke(normalized_query)

        logging.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}")
        logging.info(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©: {retrieved_docs}")

        # Few-shotê³¼ ë¬¸ì„œ ê²€ìƒ‰ ì¶©ëŒ ì—¬ë¶€ í™•ì¸
        if answer:
            validated_answer = validate_answer_with_docs(answer, retrieved_docs)
            if validated_answer:
                return validated_answer  # ì¶©ëŒ ì—†ìœ¼ë©´ Few-shot ìœ ì§€

        if retrieved_docs:
            logging.info("ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„±")
            rag_chain = get_rag_chain()
            ai_response = rag_chain.invoke(
                {"input": normalized_query},
                config={"configurable": {"session_id": session_id}},
            )
        else:
            logging.info(" ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŒ. Few-shot Prompting ì‚¬ìš©")

            # Few-shot Prompt ì‹¤í–‰ ë¡œê·¸ ì¶”ê°€
            final_prompt = faq_prompt.format(query=normalized_query)
            logging.info(f"Few-shot Prompt ì‹¤í–‰ í”„ë¡¬í”„íŠ¸: {final_prompt}")

            llm = get_llm()
            ai_response = llm(final_prompt)

            # ì‹¤í–‰ ê²°ê³¼ ë¡œê·¸ í™•ì¸
            logging.info(f" Few-shot Prompt ê²°ê³¼: {ai_response}")

        if isinstance(ai_response, dict):
            ai_response = ai_response.get("answer", "")

        # AI ì‘ë‹µì´ ì—†ì„ ê²½ìš° ê¸°ë³¸ ë©”ì‹œì§€ ì¶”ê°€
        if not ai_response or ai_response.strip() == "":
            logging.info("âš  Few-shot Prompt ì‹¤í–‰ í›„ AI ì‘ë‹µì´ ì—†ìŒ. ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ í•´ë‹¹ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œë©´ ë„ì™€ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        return ai_response.strip()

    except Exception as e:
        logging.error(f" AI ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
retriever = get_retriever()
query = "ì„œìš¸ê³¼ê¸°ëŒ€ ë¼ì´ë¸Œ ë°©ì†¡ ì´¬ì˜ ê´€ë¦¬ ë³´ìœ  ê¸°ê°„"  # í…ŒìŠ¤íŠ¸í•  ì§ˆë¬¸
retrieved_docs = retriever.invoke(query)

print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(retrieved_docs)}")

for idx, doc in enumerate(retrieved_docs[:3]):
    print(f"\n ë¬¸ì„œ {idx+1}: {doc.metadata}")
    print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{doc.page_content[:500]}...")